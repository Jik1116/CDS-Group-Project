{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import ViTImageProcessor, RobertaTokenizer, Blip2Processor, Blip2ForConditionalGeneration, InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "\n",
    "from datasets import list_metrics\n",
    "\n",
    "from diffusers import AudioLDMPipeline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d60fcc9996414fa3e7831e6c2898e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\"ybelkada/blip2-opt-2.7b-fp16-sharded\", device_map=\"auto\", load_in_8bit=True, torch_dtype=torch.float32)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, dataframe):\n",
    "#         self.dataframe = dataframe\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataframe)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         # transform = transforms.Compose([\n",
    "#         #     transforms.PILToTensor(),\n",
    "#         # ])\n",
    "#         image = Image.open(f\"images/{idx}.png\")\n",
    "#         image_features = processor(image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "#         labels = tokenizer(self.dataframe[\"caption\"][idx],return_tensors=\"pt\",\n",
    "#                                           max_length=46,\n",
    "#                                           pad_to_max_length=True,\n",
    "#                                           return_token_type_ids=True,\n",
    "#                                           truncation=True).input_ids\n",
    "#         return {'pixel_values':image_features.squeeze(0),'labels':self.dataframe[\"caption\"][idx], \"idx\":idx}\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(f\"images/{idx}.png\")\n",
    "        encoding = self.processor(images=image, return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        encoding[\"text\"] = self.dataset[\"caption\"][idx]\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate outputs to see base model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"images/1.png\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float32)\n",
    "outputs = model.generate(**inputs, max_length = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a close up of a guitar with strings and strings\n"
     ]
    }
   ],
   "source": [
    "generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add LoRA layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,406,720 || all params: 3,749,922,816 || trainable%: 0.1441821676150467\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"lora_only\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\"]\n",
    ")\n",
    "\n",
    "training_model = get_peft_model(model, config)\n",
    "training_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"musiccaps-public.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ytid</th>\n",
       "      <th>start_s</th>\n",
       "      <th>end_s</th>\n",
       "      <th>audioset_positive_labels</th>\n",
       "      <th>aspect_list</th>\n",
       "      <th>caption</th>\n",
       "      <th>author_id</th>\n",
       "      <th>is_balanced_subset</th>\n",
       "      <th>is_audioset_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0SdAVK79lg</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/0155w,/m/01lyv,/m/0342h,/m/042v_gx,/m/04rlf...</td>\n",
       "      <td>['guitar song', 'piano backing', 'simple percu...</td>\n",
       "      <td>This song features an electric guitar as the m...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1LrH01Ei1w</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/02p0sh1,/m/04rlf</td>\n",
       "      <td>['rubab instrument', 'repetitive melody on dif...</td>\n",
       "      <td>This song features a rubber instrument being p...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4NLarMj4xU</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/04rlf,/t/dd00034</td>\n",
       "      <td>['pop', 'tinny wide hi hats', 'mellow piano me...</td>\n",
       "      <td>The Pop song features a soft female vocal sing...</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5f6hjZf9Yw</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/02w4v,/m/04rlf</td>\n",
       "      <td>['folk music', 'rubab', 'male voice', 'slow te...</td>\n",
       "      <td>This folk song features a male voice singing t...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5xOcMJpTUk</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>/m/018vs,/m/0342h,/m/042v_gx,/m/04rlf,/m/04szw...</td>\n",
       "      <td>['guitarist', 'male talking', 'twang sounds', ...</td>\n",
       "      <td>A male guitarist plays the guitar and speaks a...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>zqbHYVH6Wqo</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/085jw,/m/0l14l2</td>\n",
       "      <td>['traditional horn instruments', 'devotional',...</td>\n",
       "      <td>The song is an instrumental. The tempo is medi...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>zrb76mJOZQQ</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>/m/0395lw,/m/0gy1t2s</td>\n",
       "      <td>['amateur recording', 'no music', 'sound of be...</td>\n",
       "      <td>This amateur recording features the sound of t...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>zu_1zpF--Zg</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>/m/01xqw,/m/02fsn,/m/0d8_n,/m/0l14_3</td>\n",
       "      <td>['amateur recording', 'jazz/bossa-nova', 'upri...</td>\n",
       "      <td>This audio contains someone playing jazz chord...</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>zw5dkiklbhE</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>/m/01sm1g,/m/0l14md</td>\n",
       "      <td>['amateur recording', 'percussion', 'wooden bo...</td>\n",
       "      <td>This audio contains someone playing a wooden b...</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2662</th>\n",
       "      <td>zyXa2tdBTGc</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/04rlf,/t/dd00034</td>\n",
       "      <td>['instrumental music', 'gospel music', 'strong...</td>\n",
       "      <td>The song is an instrumental. The song is slow ...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2663 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ytid  start_s  end_s  \\\n",
       "0     -0SdAVK79lg       30     40   \n",
       "1     -1LrH01Ei1w       30     40   \n",
       "2     -4NLarMj4xU       30     40   \n",
       "3     -5f6hjZf9Yw       30     40   \n",
       "4     -5xOcMJpTUk       70     80   \n",
       "...           ...      ...    ...   \n",
       "2658  zqbHYVH6Wqo       30     40   \n",
       "2659  zrb76mJOZQQ        3     13   \n",
       "2660  zu_1zpF--Zg        0     10   \n",
       "2661  zw5dkiklbhE       15     25   \n",
       "2662  zyXa2tdBTGc       30     40   \n",
       "\n",
       "                               audioset_positive_labels  \\\n",
       "0     /m/0155w,/m/01lyv,/m/0342h,/m/042v_gx,/m/04rlf...   \n",
       "1                                   /m/02p0sh1,/m/04rlf   \n",
       "2                                   /m/04rlf,/t/dd00034   \n",
       "3                                     /m/02w4v,/m/04rlf   \n",
       "4     /m/018vs,/m/0342h,/m/042v_gx,/m/04rlf,/m/04szw...   \n",
       "...                                                 ...   \n",
       "2658                                 /m/085jw,/m/0l14l2   \n",
       "2659                               /m/0395lw,/m/0gy1t2s   \n",
       "2660               /m/01xqw,/m/02fsn,/m/0d8_n,/m/0l14_3   \n",
       "2661                                /m/01sm1g,/m/0l14md   \n",
       "2662                                /m/04rlf,/t/dd00034   \n",
       "\n",
       "                                            aspect_list  \\\n",
       "0     ['guitar song', 'piano backing', 'simple percu...   \n",
       "1     ['rubab instrument', 'repetitive melody on dif...   \n",
       "2     ['pop', 'tinny wide hi hats', 'mellow piano me...   \n",
       "3     ['folk music', 'rubab', 'male voice', 'slow te...   \n",
       "4     ['guitarist', 'male talking', 'twang sounds', ...   \n",
       "...                                                 ...   \n",
       "2658  ['traditional horn instruments', 'devotional',...   \n",
       "2659  ['amateur recording', 'no music', 'sound of be...   \n",
       "2660  ['amateur recording', 'jazz/bossa-nova', 'upri...   \n",
       "2661  ['amateur recording', 'percussion', 'wooden bo...   \n",
       "2662  ['instrumental music', 'gospel music', 'strong...   \n",
       "\n",
       "                                                caption  author_id  \\\n",
       "0     This song features an electric guitar as the m...          0   \n",
       "1     This song features a rubber instrument being p...          0   \n",
       "2     The Pop song features a soft female vocal sing...          4   \n",
       "3     This folk song features a male voice singing t...          0   \n",
       "4     A male guitarist plays the guitar and speaks a...          1   \n",
       "...                                                 ...        ...   \n",
       "2658  The song is an instrumental. The tempo is medi...          1   \n",
       "2659  This amateur recording features the sound of t...          0   \n",
       "2660  This audio contains someone playing jazz chord...          6   \n",
       "2661  This audio contains someone playing a wooden b...          6   \n",
       "2662  The song is an instrumental. The song is slow ...          1   \n",
       "\n",
       "      is_balanced_subset  is_audioset_eval  \n",
       "0                  False             False  \n",
       "1                  False             False  \n",
       "2                  False             False  \n",
       "3                  False             False  \n",
       "4                  False             False  \n",
       "...                  ...               ...  \n",
       "2658               False             False  \n",
       "2659               False             False  \n",
       "2660               False             False  \n",
       "2661               False             False  \n",
       "2662               False             False  \n",
       "\n",
       "[2663 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = data[data[\"is_audioset_eval\"] == False].reset_index(drop=True)\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ytid</th>\n",
       "      <th>start_s</th>\n",
       "      <th>end_s</th>\n",
       "      <th>audioset_positive_labels</th>\n",
       "      <th>aspect_list</th>\n",
       "      <th>caption</th>\n",
       "      <th>author_id</th>\n",
       "      <th>is_balanced_subset</th>\n",
       "      <th>is_audioset_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0Gj8-vB1q4</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/0140xf,/m/02cjck,/m/04rlf</td>\n",
       "      <td>['low quality', 'sustained strings melody', 's...</td>\n",
       "      <td>The low quality recording features a ballad so...</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0vPFx-wRRI</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/025_jnm,/m/04rlf</td>\n",
       "      <td>['amateur recording', 'finger snipping', 'male...</td>\n",
       "      <td>a male voice is singing a melody with changing...</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0xzrMun0Rs</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/01g90h,/m/04rlf</td>\n",
       "      <td>['backing track', 'jazzy', 'digital drums', 'p...</td>\n",
       "      <td>This song contains digital drums playing a sim...</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1OlgJWehn8</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/04rlf,/m/06bz3</td>\n",
       "      <td>['instrumental', 'white noise', 'female vocali...</td>\n",
       "      <td>This clip is three tracks playing consecutivel...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1UWSisR2zo</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/04rlf,/m/0xzly</td>\n",
       "      <td>['live performance', 'poor audio quality', 'am...</td>\n",
       "      <td>A male singer sings this groovy melody. The so...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>zrrM6Qg2Dwg</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/04rlf,/m/0l156b</td>\n",
       "      <td>['steel pan music', 'happy mood', 'caribbean f...</td>\n",
       "      <td>This song features the main melody played on a...</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>ztfegVzqeCI</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/015lz1,/m/01v1d8,/m/04rlf,/m/07kc_,/m/0l14qv</td>\n",
       "      <td>['female singer', 'synth bass', 'reverb', 'the...</td>\n",
       "      <td>A quirky drum machine and warm synth bass prov...</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>zwfo7wnXdjs</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>/m/02p0sh1,/m/04rlf,/m/06j64v</td>\n",
       "      <td>['instrumental music', 'arabic music', 'genera...</td>\n",
       "      <td>The song is an instrumental. The song is mediu...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>zx_vcwOsDO4</td>\n",
       "      <td>50</td>\n",
       "      <td>60</td>\n",
       "      <td>/m/01glhc,/m/02sgy,/m/0342h,/m/03lty,/m/04rlf,...</td>\n",
       "      <td>['instrumental', 'no voice', 'electric guitar'...</td>\n",
       "      <td>The rock music is purely instrumental and feat...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>zzNdwF40ID8</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "      <td>/m/04rlf,/m/0790c</td>\n",
       "      <td>['glitch', 'noise', 'instrumental', 'electroni...</td>\n",
       "      <td>This is a glitch music piece. There is a synth...</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2858 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ytid  start_s  end_s  \\\n",
       "0     -0Gj8-vB1q4       30     40   \n",
       "1     -0vPFx-wRRI       30     40   \n",
       "2     -0xzrMun0Rs       30     40   \n",
       "3     -1OlgJWehn8       30     40   \n",
       "4     -1UWSisR2zo       30     40   \n",
       "...           ...      ...    ...   \n",
       "2853  zrrM6Qg2Dwg       30     40   \n",
       "2854  ztfegVzqeCI       30     40   \n",
       "2855  zwfo7wnXdjs       30     40   \n",
       "2856  zx_vcwOsDO4       50     60   \n",
       "2857  zzNdwF40ID8       70     80   \n",
       "\n",
       "                               audioset_positive_labels  \\\n",
       "0                          /m/0140xf,/m/02cjck,/m/04rlf   \n",
       "1                                   /m/025_jnm,/m/04rlf   \n",
       "2                                    /m/01g90h,/m/04rlf   \n",
       "3                                     /m/04rlf,/m/06bz3   \n",
       "4                                     /m/04rlf,/m/0xzly   \n",
       "...                                                 ...   \n",
       "2853                                 /m/04rlf,/m/0l156b   \n",
       "2854    /m/015lz1,/m/01v1d8,/m/04rlf,/m/07kc_,/m/0l14qv   \n",
       "2855                      /m/02p0sh1,/m/04rlf,/m/06j64v   \n",
       "2856  /m/01glhc,/m/02sgy,/m/0342h,/m/03lty,/m/04rlf,...   \n",
       "2857                                  /m/04rlf,/m/0790c   \n",
       "\n",
       "                                            aspect_list  \\\n",
       "0     ['low quality', 'sustained strings melody', 's...   \n",
       "1     ['amateur recording', 'finger snipping', 'male...   \n",
       "2     ['backing track', 'jazzy', 'digital drums', 'p...   \n",
       "3     ['instrumental', 'white noise', 'female vocali...   \n",
       "4     ['live performance', 'poor audio quality', 'am...   \n",
       "...                                                 ...   \n",
       "2853  ['steel pan music', 'happy mood', 'caribbean f...   \n",
       "2854  ['female singer', 'synth bass', 'reverb', 'the...   \n",
       "2855  ['instrumental music', 'arabic music', 'genera...   \n",
       "2856  ['instrumental', 'no voice', 'electric guitar'...   \n",
       "2857  ['glitch', 'noise', 'instrumental', 'electroni...   \n",
       "\n",
       "                                                caption  author_id  \\\n",
       "0     The low quality recording features a ballad so...          4   \n",
       "1     a male voice is singing a melody with changing...          6   \n",
       "2     This song contains digital drums playing a sim...          6   \n",
       "3     This clip is three tracks playing consecutivel...          7   \n",
       "4     A male singer sings this groovy melody. The so...          1   \n",
       "...                                                 ...        ...   \n",
       "2853  This song features the main melody played on a...          0   \n",
       "2854  A quirky drum machine and warm synth bass prov...          8   \n",
       "2855  The song is an instrumental. The song is mediu...          1   \n",
       "2856  The rock music is purely instrumental and feat...          2   \n",
       "2857  This is a glitch music piece. There is a synth...          9   \n",
       "\n",
       "      is_balanced_subset  is_audioset_eval  \n",
       "0                  False              True  \n",
       "1                  False              True  \n",
       "2                  False              True  \n",
       "3                  False              True  \n",
       "4                  False              True  \n",
       "...                  ...               ...  \n",
       "2853                True              True  \n",
       "2854                True              True  \n",
       "2855                True              True  \n",
       "2856                True              True  \n",
       "2857                True              True  \n",
       "\n",
       "[2858 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_data = data[data[\"is_audioset_eval\"] == True].reset_index(drop=True)\n",
    "display(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = CustomDataset(train_data, processor)\n",
    "val_data = CustomDataset(val_data, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # pad the input_ids and attention_mask\n",
    "    processed_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key != \"text\":\n",
    "            processed_batch[key] = torch.stack([example[key] for example in batch])\n",
    "        else:\n",
    "            text_inputs = processor.tokenizer(\n",
    "                [example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n",
    "            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
    "    return processed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=3, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 2.2588915824890137\n",
      "Loss: 1.9077609777450562\n",
      "Loss: 2.089205503463745\n",
      "Loss: 1.6575348377227783\n",
      "Loss: 2.530764102935791\n",
      "Loss: 1.530328631401062\n",
      "Loss: 1.9788475036621094\n",
      "Loss: 1.7268588542938232\n",
      "Loss: 2.0601391792297363\n",
      "Loss: 2.533592939376831\n",
      "Loss: 1.9488040208816528\n",
      "Loss: 1.663870096206665\n",
      "Loss: 2.2682833671569824\n",
      "Loss: 1.4550635814666748\n",
      "Loss: 1.8368775844573975\n",
      "Loss: 2.1807525157928467\n",
      "Loss: 1.6045293807983398\n",
      "Loss: 1.6954013109207153\n",
      "Loss: 1.535036325454712\n",
      "Loss: 1.625605821609497\n",
      "Loss: 2.015024423599243\n",
      "Loss: 1.7804661989212036\n",
      "Loss: 2.141453504562378\n",
      "Loss: 2.0120861530303955\n",
      "Loss: 2.4234282970428467\n",
      "Loss: 1.5865806341171265\n",
      "Loss: 1.6444388628005981\n",
      "Loss: 1.7841315269470215\n",
      "Loss: 2.5273563861846924\n",
      "Loss: 1.6347119808197021\n",
      "Loss: 1.6874228715896606\n",
      "Loss: 1.364664077758789\n",
      "Loss: 2.335240125656128\n",
      "Loss: 1.71576988697052\n",
      "Loss: 2.1358940601348877\n",
      "Loss: 2.124706745147705\n",
      "Loss: 1.608621597290039\n",
      "Loss: 2.097714424133301\n",
      "Loss: 2.098231792449951\n",
      "Loss: 2.0292553901672363\n",
      "Loss: 2.287999153137207\n",
      "Loss: 2.581772565841675\n",
      "Loss: 2.002041816711426\n",
      "Loss: 2.253114700317383\n",
      "Loss: 2.401203155517578\n",
      "Loss: 2.167156934738159\n",
      "Loss: 2.4476161003112793\n",
      "Loss: 2.4884824752807617\n",
      "Loss: 1.6813387870788574\n",
      "Loss: 1.9420207738876343\n",
      "Loss: 2.3086326122283936\n",
      "Loss: 1.6978824138641357\n",
      "Loss: 1.351928472518921\n",
      "Loss: 1.735877275466919\n",
      "Loss: 1.2894290685653687\n",
      "Loss: 2.1797194480895996\n",
      "Loss: 1.9394515752792358\n",
      "Loss: 1.9608285427093506\n",
      "Loss: 1.9757589101791382\n",
      "Loss: 1.9309327602386475\n",
      "Loss: 1.69636070728302\n",
      "Loss: 1.7303450107574463\n",
      "Loss: 1.8898707628250122\n",
      "Loss: 1.8071948289871216\n",
      "Loss: 1.601935863494873\n",
      "Loss: 1.5280921459197998\n",
      "Loss: 1.4719808101654053\n",
      "Loss: 1.805820107460022\n",
      "Loss: 1.6017167568206787\n",
      "Loss: 1.5382155179977417\n",
      "Loss: 1.561417818069458\n",
      "Loss: 2.598090410232544\n",
      "Loss: 1.9757401943206787\n",
      "Loss: 1.9383008480072021\n",
      "Loss: 1.7386798858642578\n",
      "Loss: 1.7859357595443726\n",
      "Loss: 2.182451009750366\n",
      "Loss: 1.9416284561157227\n",
      "Loss: 1.773453950881958\n",
      "Loss: 1.8415497541427612\n",
      "Loss: 2.047834873199463\n",
      "Loss: 1.7970788478851318\n",
      "Loss: 1.890869140625\n",
      "Loss: 1.1871004104614258\n",
      "Loss: 1.5421168804168701\n",
      "Loss: 2.254185676574707\n",
      "Loss: 2.3044886589050293\n",
      "Loss: 2.312143325805664\n",
      "Loss: 1.5576916933059692\n",
      "Loss: 1.8310972452163696\n",
      "Loss: 1.9040472507476807\n",
      "Loss: 2.1251578330993652\n",
      "Loss: 2.083564281463623\n",
      "Loss: 2.496473789215088\n",
      "Loss: 1.8433130979537964\n",
      "Loss: 2.0320076942443848\n",
      "Loss: 1.713071346282959\n",
      "Loss: 1.3820894956588745\n",
      "Loss: 2.0548934936523438\n",
      "Loss: 2.7772388458251953\n",
      "Loss: 1.8332666158676147\n",
      "Loss: 1.564631462097168\n",
      "Loss: 1.327352523803711\n",
      "Loss: 2.395673990249634\n",
      "Loss: 1.4902019500732422\n",
      "Loss: 1.7312328815460205\n",
      "Loss: 1.8728513717651367\n",
      "Loss: 2.061011791229248\n",
      "Loss: 2.206495761871338\n",
      "Loss: 1.8605365753173828\n",
      "Loss: 1.4793524742126465\n",
      "Loss: 2.0788936614990234\n",
      "Loss: 1.663556456565857\n",
      "Loss: 1.5433038473129272\n",
      "Loss: 2.0756192207336426\n",
      "Loss: 2.562335729598999\n",
      "Loss: 1.856914758682251\n",
      "Loss: 1.9322047233581543\n",
      "Loss: 1.7382348775863647\n",
      "Loss: 1.6526525020599365\n",
      "Loss: 2.135937213897705\n",
      "Loss: 2.2514054775238037\n",
      "Loss: 1.6588596105575562\n",
      "Loss: 1.6721166372299194\n",
      "Loss: 1.9157441854476929\n",
      "Loss: 2.3045270442962646\n",
      "Loss: 1.5514723062515259\n",
      "Loss: 1.5673110485076904\n",
      "Loss: 1.855859637260437\n",
      "Loss: 1.5273256301879883\n",
      "Loss: 1.4713404178619385\n",
      "Loss: 1.824325442314148\n",
      "Loss: 1.8559366464614868\n",
      "Loss: 1.459879994392395\n",
      "Loss: 2.3320043087005615\n",
      "Loss: 1.7170047760009766\n",
      "Loss: 2.084312915802002\n",
      "Loss: 1.8202953338623047\n",
      "Loss: 2.064966917037964\n",
      "Loss: 1.41441810131073\n",
      "Loss: 2.2980778217315674\n",
      "Loss: 1.7072193622589111\n",
      "Loss: 2.449150800704956\n",
      "Loss: 1.7105618715286255\n",
      "Loss: 1.3484457731246948\n",
      "Loss: 1.8573275804519653\n",
      "Loss: 1.355582356452942\n",
      "Loss: 1.729220986366272\n",
      "Loss: 1.614598035812378\n",
      "Loss: 1.9805914163589478\n",
      "Loss: 2.641101837158203\n",
      "Loss: 1.4856542348861694\n",
      "Loss: 2.5401062965393066\n",
      "Loss: 1.9077576398849487\n",
      "Loss: 2.153261423110962\n",
      "Loss: 1.544374942779541\n",
      "Loss: 1.6133549213409424\n",
      "Loss: 1.300319790840149\n",
      "Loss: 2.0840377807617188\n",
      "Loss: 2.0393729209899902\n",
      "Loss: 1.8318767547607422\n",
      "Loss: 1.741613507270813\n",
      "Loss: 1.9828444719314575\n",
      "Loss: 2.7536463737487793\n",
      "Loss: 1.6496522426605225\n",
      "Loss: 1.315800666809082\n",
      "Loss: 2.1139252185821533\n",
      "Loss: 1.4968841075897217\n",
      "Loss: 1.8219279050827026\n",
      "Loss: 2.0857632160186768\n",
      "Loss: 2.2097809314727783\n",
      "Loss: 1.8250066041946411\n",
      "Loss: 1.4879090785980225\n",
      "Loss: 2.0903518199920654\n",
      "Loss: 1.8864537477493286\n",
      "Loss: 1.649743914604187\n",
      "Loss: 2.257009267807007\n",
      "Loss: 1.495705246925354\n",
      "Loss: 1.9478822946548462\n",
      "Loss: 1.5925098657608032\n",
      "Loss: 2.4718966484069824\n",
      "Loss: 1.8978275060653687\n",
      "Loss: 1.6444008350372314\n",
      "Loss: 1.5292186737060547\n",
      "Loss: 1.8625904321670532\n",
      "Loss: 1.580741047859192\n",
      "Loss: 1.4003044366836548\n",
      "Loss: 1.5607311725616455\n",
      "Loss: 2.054072380065918\n",
      "Loss: 2.2123427391052246\n",
      "Loss: 1.41535222530365\n",
      "Loss: 1.8703315258026123\n",
      "Loss: 2.3168725967407227\n",
      "Loss: 1.9669625759124756\n",
      "Loss: 1.7444452047348022\n",
      "Loss: 2.152033567428589\n",
      "Loss: 1.8171679973602295\n",
      "Loss: 1.660559058189392\n",
      "Loss: 1.405892014503479\n",
      "Loss: 1.9909961223602295\n",
      "Loss: 1.9017645120620728\n",
      "Loss: 1.4627716541290283\n",
      "Loss: 1.155265212059021\n",
      "Loss: 2.2205543518066406\n",
      "Loss: 2.1264591217041016\n",
      "Loss: 2.052158832550049\n",
      "Loss: 1.8984951972961426\n",
      "Loss: 2.2517290115356445\n",
      "Loss: 1.6291121244430542\n",
      "Loss: 1.9179939031600952\n",
      "Loss: 1.277496337890625\n",
      "Loss: 1.7705270051956177\n",
      "Loss: 1.8543355464935303\n",
      "Loss: 1.5291210412979126\n",
      "Loss: 1.6988931894302368\n",
      "Loss: 1.7821847200393677\n",
      "Loss: 1.7843854427337646\n",
      "Loss: 1.7772724628448486\n",
      "Loss: 2.0747687816619873\n",
      "Loss: 1.4782631397247314\n",
      "Loss: 1.4409936666488647\n",
      "Loss: 1.8109608888626099\n",
      "Loss: 2.2703847885131836\n",
      "Loss: 1.6398775577545166\n",
      "Loss: 2.203859806060791\n",
      "Loss: 1.6574348211288452\n",
      "Loss: 1.918774962425232\n",
      "Loss: 1.4281216859817505\n",
      "Loss: 1.6847865581512451\n",
      "Loss: 1.8618659973144531\n",
      "Loss: 1.6389625072479248\n",
      "Loss: 1.7433180809020996\n",
      "Loss: 1.8375087976455688\n",
      "Loss: 1.817460060119629\n",
      "Loss: 2.096597909927368\n",
      "Loss: 1.7216428518295288\n",
      "Loss: 1.6266371011734009\n",
      "Loss: 1.981332540512085\n",
      "Loss: 2.0917816162109375\n",
      "Loss: 2.157390832901001\n",
      "Loss: 2.151380777359009\n",
      "Loss: 2.361632823944092\n",
      "Loss: 1.848914623260498\n",
      "Loss: 1.7030457258224487\n",
      "Loss: 1.5879707336425781\n",
      "Loss: 1.7938989400863647\n",
      "Loss: 1.762137770652771\n",
      "Loss: 1.1064207553863525\n",
      "Loss: 1.9529906511306763\n",
      "Loss: 1.2073103189468384\n",
      "Loss: 2.3328909873962402\n",
      "Loss: 1.8782532215118408\n",
      "Loss: 1.953159213066101\n",
      "Loss: 1.730932593345642\n",
      "Loss: 1.7990052700042725\n",
      "Loss: 1.6445538997650146\n",
      "Loss: 1.619226098060608\n",
      "Loss: 1.5860404968261719\n",
      "Loss: 2.157618761062622\n",
      "Loss: 1.9022785425186157\n",
      "Loss: 2.113313913345337\n",
      "Loss: 1.7861772775650024\n",
      "Loss: 2.3341543674468994\n",
      "Loss: 2.1735002994537354\n",
      "Loss: 1.123975396156311\n",
      "Loss: 1.5805190801620483\n",
      "Loss: 2.072779417037964\n",
      "Loss: 2.3099584579467773\n",
      "Loss: 1.854343295097351\n",
      "Loss: 2.0079400539398193\n",
      "Loss: 2.1447858810424805\n",
      "Loss: 1.4834924936294556\n",
      "Loss: 1.8855369091033936\n",
      "Loss: 1.9128360748291016\n",
      "Loss: 1.8545016050338745\n",
      "Loss: 1.8674306869506836\n",
      "Loss: 2.1334617137908936\n",
      "Loss: 1.7864580154418945\n",
      "Loss: 1.9063445329666138\n",
      "Loss: 1.794147253036499\n",
      "Loss: 1.8448266983032227\n",
      "Loss: 1.4458034038543701\n",
      "Loss: 1.7848278284072876\n",
      "Loss: 1.9644931554794312\n",
      "Loss: 1.8968170881271362\n",
      "Loss: 1.5436739921569824\n",
      "Loss: 1.7807250022888184\n",
      "Loss: 1.6249247789382935\n",
      "Loss: 1.9221466779708862\n",
      "Loss: 1.4594783782958984\n",
      "Loss: 1.282546877861023\n",
      "Loss: 2.187804937362671\n",
      "Loss: 2.1329264640808105\n",
      "Loss: 2.4349515438079834\n",
      "Loss: 1.9770301580429077\n",
      "Loss: 1.5337581634521484\n",
      "Loss: 1.8568618297576904\n",
      "Loss: 2.5719077587127686\n",
      "Loss: 1.6568208932876587\n",
      "Loss: 1.3826532363891602\n",
      "Loss: 1.3682128190994263\n",
      "Loss: 1.943129062652588\n",
      "Loss: 1.9550446271896362\n",
      "Loss: 2.044692277908325\n",
      "Loss: 1.967970848083496\n",
      "Loss: 2.0427088737487793\n",
      "Loss: 1.4907665252685547\n",
      "Loss: 1.4033397436141968\n",
      "Loss: 2.3496744632720947\n",
      "Loss: 1.5064921379089355\n",
      "Loss: 2.36258864402771\n",
      "Loss: 1.8850882053375244\n",
      "Loss: 1.5957001447677612\n",
      "Loss: 2.0318686962127686\n",
      "Loss: 2.2265524864196777\n",
      "Loss: 1.847574234008789\n",
      "Loss: 2.0228970050811768\n",
      "Loss: 1.971632480621338\n",
      "Loss: 1.426667332649231\n",
      "Loss: 1.5781211853027344\n",
      "Loss: 1.9075722694396973\n",
      "Loss: 1.649408221244812\n",
      "Loss: 1.853338599205017\n",
      "Loss: 1.6635639667510986\n",
      "Loss: 1.817409873008728\n",
      "Loss: 2.0160107612609863\n",
      "Loss: 1.4467498064041138\n",
      "Loss: 1.7553199529647827\n",
      "Loss: 2.076899528503418\n",
      "Loss: 1.909414291381836\n",
      "Loss: 1.3378926515579224\n",
      "Loss: 1.9139463901519775\n",
      "Loss: 2.0764496326446533\n",
      "Loss: 2.0681941509246826\n",
      "Loss: 1.972702145576477\n",
      "Loss: 1.606566309928894\n",
      "Loss: 2.5068721771240234\n",
      "Loss: 1.7587476968765259\n",
      "Loss: 1.5321589708328247\n",
      "Loss: 1.952936053276062\n",
      "Loss: 2.2238404750823975\n",
      "Loss: 2.4283154010772705\n",
      "Loss: 2.209731340408325\n",
      "Loss: 2.2971138954162598\n",
      "Loss: 1.7309678792953491\n",
      "Loss: 2.510627269744873\n",
      "Loss: 1.7200708389282227\n",
      "Loss: 2.310140371322632\n",
      "Loss: 1.6298528909683228\n",
      "Loss: 1.7610583305358887\n",
      "Loss: 1.8264412879943848\n",
      "Loss: 1.91824209690094\n",
      "Loss: 1.8555731773376465\n",
      "Loss: 1.6025116443634033\n",
      "Loss: 1.7636125087738037\n",
      "Loss: 1.6520835161209106\n",
      "Loss: 1.503868818283081\n",
      "Loss: 1.7492069005966187\n",
      "Loss: 1.522515892982483\n",
      "Loss: 1.8270663022994995\n",
      "Loss: 1.7844125032424927\n",
      "Loss: 1.9645460844039917\n",
      "Loss: 1.681328535079956\n",
      "Loss: 2.1670525074005127\n",
      "Loss: 1.853560209274292\n",
      "Loss: 2.4635469913482666\n",
      "Loss: 1.948103666305542\n",
      "Loss: 1.5979689359664917\n",
      "Loss: 1.5050166845321655\n",
      "Loss: 1.4847352504730225\n",
      "Loss: 1.317765474319458\n",
      "Loss: 2.355381727218628\n",
      "Loss: 1.4378893375396729\n",
      "Loss: 1.6114758253097534\n",
      "Loss: 1.8439544439315796\n",
      "Loss: 1.3404887914657593\n",
      "Loss: 1.612987995147705\n",
      "Loss: 1.4590685367584229\n",
      "Loss: 1.6228303909301758\n",
      "Loss: 2.091259002685547\n",
      "Loss: 1.8805670738220215\n",
      "Loss: 1.543330192565918\n",
      "Loss: 1.8355127573013306\n",
      "Loss: 1.7428600788116455\n",
      "Loss: 1.7674204111099243\n",
      "Loss: 1.1662898063659668\n",
      "Loss: 1.2335063219070435\n",
      "Loss: 1.4374386072158813\n",
      "Loss: 2.263519525527954\n",
      "Loss: 1.6265236139297485\n",
      "Loss: 1.7181612253189087\n",
      "Loss: 1.160414695739746\n",
      "Loss: 1.6948782205581665\n",
      "Loss: 1.5282002687454224\n",
      "Loss: 1.382857084274292\n",
      "Loss: 1.960647702217102\n",
      "Loss: 2.239773988723755\n",
      "Loss: 1.6777973175048828\n",
      "Loss: 1.7403545379638672\n",
      "Loss: 1.5654603242874146\n",
      "Loss: 2.291733503341675\n",
      "Loss: 1.5971996784210205\n",
      "Loss: 1.5743224620819092\n",
      "Loss: 1.8083961009979248\n",
      "Loss: 2.126786231994629\n",
      "Loss: 1.5770984888076782\n",
      "Loss: 1.7094035148620605\n",
      "Loss: 1.4566731452941895\n",
      "Loss: 2.673457145690918\n",
      "Loss: 1.629388451576233\n",
      "Loss: 1.841452956199646\n",
      "Loss: 1.8812321424484253\n",
      "Loss: 1.572473406791687\n",
      "Loss: 1.3694746494293213\n",
      "Loss: 2.3003382682800293\n",
      "Loss: 1.8284834623336792\n",
      "Loss: 1.5388847589492798\n",
      "Loss: 2.0575146675109863\n",
      "Loss: 1.741268515586853\n",
      "Loss: 2.399831533432007\n",
      "Loss: 1.626895785331726\n",
      "Loss: 1.6056994199752808\n",
      "Loss: 2.07015323638916\n",
      "Loss: 2.0631909370422363\n",
      "Loss: 1.8010199069976807\n",
      "Loss: 1.6200050115585327\n",
      "Loss: 1.6550358533859253\n",
      "Loss: 1.6552435159683228\n",
      "Loss: 1.6668745279312134\n",
      "Loss: 1.5230133533477783\n",
      "Loss: 2.1775338649749756\n",
      "Loss: 1.9811264276504517\n",
      "Loss: 2.119755744934082\n",
      "Loss: 2.0630600452423096\n",
      "Loss: 1.124884009361267\n",
      "Loss: 1.8874508142471313\n",
      "Loss: 1.7143676280975342\n",
      "Loss: 1.7914774417877197\n",
      "Loss: 1.651118278503418\n",
      "Loss: 1.4225984811782837\n",
      "Loss: 1.91323983669281\n",
      "Loss: 1.7098332643508911\n",
      "Loss: 1.6432225704193115\n",
      "Loss: 1.860780954360962\n",
      "Loss: 1.3145169019699097\n",
      "Loss: 1.549921989440918\n",
      "Loss: 2.603736639022827\n",
      "Loss: 1.9114875793457031\n",
      "Loss: 2.033060073852539\n",
      "Loss: 2.1976466178894043\n",
      "Loss: 1.8936517238616943\n",
      "Loss: 1.4198867082595825\n",
      "Loss: 1.8016221523284912\n",
      "Loss: 1.5488200187683105\n",
      "Loss: 1.7477689981460571\n",
      "Loss: 1.510279655456543\n",
      "Loss: 1.743085503578186\n",
      "Loss: 2.120274066925049\n",
      "Loss: 1.520734429359436\n",
      "Loss: 1.7156628370285034\n",
      "Loss: 1.7536743879318237\n",
      "Loss: 1.9305694103240967\n",
      "Loss: 1.6420317888259888\n",
      "Loss: 1.5336867570877075\n",
      "Loss: 1.6378073692321777\n",
      "Loss: 1.657826542854309\n",
      "Loss: 1.690142035484314\n",
      "Loss: 1.6824206113815308\n",
      "Loss: 2.32828688621521\n",
      "Loss: 2.1165080070495605\n",
      "Loss: 1.6775633096694946\n",
      "Loss: 2.0220651626586914\n",
      "Loss: 2.40712571144104\n",
      "Loss: 1.420055627822876\n",
      "Loss: 2.063189744949341\n",
      "Loss: 1.8077439069747925\n",
      "Loss: 1.3720431327819824\n",
      "Loss: 1.909355640411377\n",
      "Loss: 1.6636449098587036\n",
      "Loss: 2.5486156940460205\n",
      "Loss: 2.2636590003967285\n",
      "Loss: 2.0859336853027344\n",
      "Loss: 2.285085439682007\n",
      "Loss: 1.3723548650741577\n",
      "Loss: 1.8322190046310425\n",
      "Loss: 1.5593830347061157\n",
      "Loss: 1.5536679029464722\n",
      "Loss: 1.7990891933441162\n",
      "Loss: 2.1961047649383545\n",
      "Loss: 1.8736108541488647\n",
      "Loss: 1.616686224937439\n",
      "Loss: 1.5823097229003906\n",
      "Loss: 2.013279676437378\n",
      "Loss: 2.0247111320495605\n",
      "Loss: 1.4258642196655273\n",
      "Loss: 1.6334153413772583\n",
      "Loss: 2.2509562969207764\n",
      "Loss: 2.255328893661499\n",
      "Loss: 1.8779079914093018\n",
      "Loss: 1.1547160148620605\n",
      "Loss: 2.000430107116699\n",
      "Loss: 2.05216646194458\n",
      "Loss: 2.225767135620117\n",
      "Loss: 1.7821701765060425\n",
      "Loss: 1.8034108877182007\n",
      "Loss: 1.613926887512207\n",
      "Loss: 1.6365288496017456\n",
      "Loss: 2.1324219703674316\n",
      "Loss: 2.9083993434906006\n",
      "Loss: 1.707961082458496\n",
      "Loss: 1.658700942993164\n",
      "Loss: 2.1359317302703857\n",
      "Loss: 1.391215205192566\n",
      "Loss: 2.0326144695281982\n",
      "Loss: 1.6046545505523682\n",
      "Loss: 2.048110008239746\n",
      "Loss: 1.7805190086364746\n",
      "Loss: 2.034891128540039\n",
      "Loss: 2.4594573974609375\n",
      "Loss: 1.7388641834259033\n",
      "Loss: 1.7480347156524658\n",
      "Loss: 1.267980694770813\n",
      "Loss: 2.073939561843872\n",
      "Loss: 2.0367510318756104\n",
      "Loss: 1.7385518550872803\n",
      "Loss: 1.6522694826126099\n",
      "Loss: 1.834449291229248\n",
      "Loss: 1.6955997943878174\n",
      "Loss: 1.3587206602096558\n",
      "Loss: 1.772647738456726\n",
      "Loss: 1.8297697305679321\n",
      "Loss: 1.1379203796386719\n",
      "Loss: 1.1921247243881226\n",
      "Loss: 1.6919498443603516\n",
      "Loss: 1.6913491487503052\n",
      "Loss: 1.8446353673934937\n",
      "Loss: 1.5262131690979004\n",
      "Loss: 1.8208752870559692\n",
      "Loss: 1.8175795078277588\n",
      "Loss: 1.8618831634521484\n",
      "Loss: 1.6985775232315063\n",
      "Loss: 1.4223164319992065\n",
      "Loss: 1.7214139699935913\n",
      "Loss: 1.6953850984573364\n",
      "Loss: 1.806494951248169\n",
      "Loss: 1.9600237607955933\n",
      "Loss: 1.4238466024398804\n",
      "Loss: 1.2431674003601074\n",
      "Loss: 1.7270406484603882\n",
      "Loss: 1.314606785774231\n",
      "Loss: 1.4921190738677979\n",
      "Loss: 2.083315372467041\n",
      "Loss: 1.5069119930267334\n",
      "Loss: 1.482743740081787\n",
      "Loss: 1.9263567924499512\n",
      "Loss: 2.1413636207580566\n",
      "Loss: 1.1374313831329346\n",
      "Loss: 1.9206165075302124\n",
      "Loss: 2.289454698562622\n",
      "Loss: 1.6143178939819336\n",
      "Loss: 1.6036343574523926\n",
      "Loss: 1.5928566455841064\n",
      "Loss: 1.8477201461791992\n",
      "Loss: 1.5903164148330688\n",
      "Loss: 2.1735081672668457\n",
      "Loss: 1.7476118803024292\n",
      "Loss: 1.8847054243087769\n",
      "Loss: 1.7148160934448242\n",
      "Loss: 1.5897655487060547\n",
      "Loss: 1.3775748014450073\n",
      "Loss: 2.1643176078796387\n",
      "Loss: 1.3860797882080078\n",
      "Loss: 1.4369829893112183\n",
      "Loss: 1.8838225603103638\n",
      "Loss: 1.770749807357788\n",
      "Loss: 1.1984813213348389\n",
      "Loss: 1.7418842315673828\n",
      "Loss: 1.5043293237686157\n",
      "Loss: 1.6216096878051758\n",
      "Loss: 1.819832682609558\n",
      "Loss: 1.6901285648345947\n",
      "Loss: 1.902588963508606\n",
      "Loss: 2.0079197883605957\n",
      "Loss: 1.525746464729309\n",
      "Loss: 1.7261110544204712\n",
      "Loss: 2.661898612976074\n",
      "Loss: 1.6527419090270996\n",
      "Loss: 2.5577383041381836\n",
      "Loss: 1.5922828912734985\n",
      "Loss: 1.4202834367752075\n",
      "Loss: 1.4170678853988647\n",
      "Loss: 1.441074252128601\n",
      "Loss: 1.5432813167572021\n",
      "Loss: 1.5018872022628784\n",
      "Loss: 1.265589952468872\n",
      "Loss: 2.0002055168151855\n",
      "Loss: 1.5872732400894165\n",
      "Loss: 1.6079246997833252\n",
      "Loss: 1.978825330734253\n",
      "Loss: 1.5020629167556763\n",
      "Loss: 1.2039610147476196\n",
      "Loss: 1.5189845561981201\n",
      "Loss: 1.5913305282592773\n",
      "Loss: 1.8566886186599731\n",
      "Loss: 2.0314018726348877\n",
      "Loss: 1.5466029644012451\n",
      "Loss: 1.397472620010376\n",
      "Loss: 1.7286888360977173\n",
      "Loss: 2.4586446285247803\n",
      "Loss: 1.37325119972229\n",
      "Loss: 1.9184304475784302\n",
      "Loss: 1.8899592161178589\n",
      "Loss: 1.4815459251403809\n",
      "Loss: 1.5568623542785645\n",
      "Loss: 1.4785690307617188\n",
      "Loss: 1.8900083303451538\n",
      "Loss: 1.454641580581665\n",
      "Loss: 1.6709294319152832\n",
      "Loss: 1.7391061782836914\n",
      "Loss: 1.241903305053711\n",
      "Loss: 1.6563971042633057\n",
      "Loss: 1.777082085609436\n",
      "Loss: 1.7666627168655396\n",
      "Loss: 1.703188180923462\n",
      "Loss: 1.7396000623703003\n",
      "Loss: 1.7660539150238037\n",
      "Loss: 1.6900477409362793\n",
      "Loss: 1.4097681045532227\n",
      "Loss: 1.8134571313858032\n",
      "Loss: 1.7952345609664917\n",
      "Loss: 2.0767834186553955\n",
      "Loss: 1.4159727096557617\n",
      "Loss: 1.7576227188110352\n",
      "Loss: 1.9414762258529663\n",
      "Loss: 1.6969499588012695\n",
      "Loss: 1.5608209371566772\n",
      "Loss: 2.3782689571380615\n",
      "Loss: 2.1537227630615234\n",
      "Loss: 1.48931086063385\n",
      "Loss: 1.637128472328186\n",
      "Loss: 2.1706953048706055\n",
      "Loss: 2.123213529586792\n",
      "Loss: 1.9813036918640137\n",
      "Loss: 1.5978035926818848\n",
      "Loss: 1.3326069116592407\n",
      "Loss: 1.7598800659179688\n",
      "Loss: 2.2003188133239746\n",
      "Loss: 1.947167158126831\n",
      "Loss: 2.119656562805176\n",
      "Loss: 1.8028162717819214\n",
      "Loss: 2.2287235260009766\n",
      "Loss: 1.6570972204208374\n",
      "Loss: 1.9704285860061646\n",
      "Loss: 1.5202735662460327\n",
      "Loss: 1.6385023593902588\n",
      "Loss: 1.3471043109893799\n",
      "Loss: 1.5913810729980469\n",
      "Loss: 1.492817997932434\n",
      "Loss: 2.154109001159668\n",
      "Loss: 1.5266104936599731\n",
      "Loss: 1.9103680849075317\n",
      "Loss: 1.8287216424942017\n",
      "Loss: 1.9668545722961426\n",
      "Loss: 1.9953845739364624\n",
      "Loss: 1.6611111164093018\n",
      "Loss: 1.6073267459869385\n",
      "Loss: 1.6910934448242188\n",
      "Loss: 2.0492429733276367\n",
      "Loss: 1.8268033266067505\n",
      "Loss: 1.712357997894287\n",
      "Loss: 2.8487095832824707\n",
      "Loss: 1.0090745687484741\n",
      "Loss: 1.7593986988067627\n",
      "Loss: 1.9904710054397583\n",
      "Loss: 1.343400478363037\n",
      "Loss: 1.5708460807800293\n",
      "Loss: 2.024474620819092\n",
      "Loss: 1.8016761541366577\n",
      "Loss: 1.5908979177474976\n",
      "Loss: 1.4881086349487305\n",
      "Loss: 1.6572431325912476\n",
      "Loss: 1.932512640953064\n",
      "Loss: 1.5302366018295288\n",
      "Loss: 1.380509853363037\n",
      "Loss: 1.6426796913146973\n",
      "Loss: 1.6053544282913208\n",
      "Loss: 1.6282553672790527\n",
      "Loss: 2.0644803047180176\n",
      "Loss: 2.0217747688293457\n",
      "Loss: 2.2442214488983154\n",
      "Loss: 1.392802119255066\n",
      "Loss: 2.312910795211792\n",
      "Loss: 1.4543005228042603\n",
      "Loss: 1.2167999744415283\n",
      "Loss: 1.7673231363296509\n",
      "Loss: 1.5409042835235596\n",
      "Loss: 1.5218935012817383\n",
      "Loss: 1.2955418825149536\n",
      "Loss: 1.17617666721344\n",
      "Loss: 1.973766803741455\n",
      "Loss: 1.996621012687683\n",
      "Loss: 1.7256121635437012\n",
      "Loss: 1.4685187339782715\n",
      "Loss: 1.730613112449646\n",
      "Loss: 1.835329532623291\n",
      "Loss: 1.7854715585708618\n",
      "Loss: 1.511182188987732\n",
      "Loss: 1.5603572130203247\n",
      "Loss: 1.4433348178863525\n",
      "Loss: 1.6187852621078491\n",
      "Loss: 1.6041240692138672\n",
      "Loss: 1.617415428161621\n",
      "Loss: 1.708179235458374\n",
      "Loss: 1.9659771919250488\n",
      "Loss: 1.5554848909378052\n",
      "Loss: 1.3298078775405884\n",
      "Loss: 1.7874025106430054\n",
      "Loss: 1.3894598484039307\n",
      "Loss: 1.5765267610549927\n",
      "Loss: 1.0170249938964844\n",
      "Loss: 1.2354487180709839\n",
      "Loss: 1.8509578704833984\n",
      "Loss: 1.5302523374557495\n",
      "Loss: 1.2635973691940308\n",
      "Loss: 1.7024686336517334\n",
      "Loss: 1.3075824975967407\n",
      "Loss: 1.371957540512085\n",
      "Loss: 2.1048507690429688\n",
      "Loss: 1.4864084720611572\n",
      "Loss: 1.9130252599716187\n",
      "Loss: 1.7901941537857056\n",
      "Loss: 1.1162545680999756\n",
      "Loss: 1.3624244928359985\n",
      "Loss: 1.4271869659423828\n",
      "Loss: 1.2046786546707153\n",
      "Loss: 1.3897244930267334\n",
      "Loss: 2.1162166595458984\n",
      "Loss: 1.9386720657348633\n",
      "Loss: 1.3337678909301758\n",
      "Loss: 1.8905104398727417\n",
      "Loss: 1.344870924949646\n",
      "Loss: 1.8416459560394287\n",
      "Loss: 1.6912082433700562\n",
      "Loss: 1.8858762979507446\n",
      "Loss: 1.4611092805862427\n",
      "Loss: 1.453669786453247\n",
      "Loss: 1.5472638607025146\n",
      "Loss: 1.474302887916565\n",
      "Loss: 1.5382298231124878\n",
      "Loss: 1.9289300441741943\n",
      "Loss: 1.53629732131958\n",
      "Loss: 1.5369306802749634\n",
      "Loss: 0.985585629940033\n",
      "Loss: 1.2508947849273682\n",
      "Loss: 1.9486067295074463\n",
      "Loss: 2.124234199523926\n",
      "Loss: 1.5685619115829468\n",
      "Loss: 1.5560656785964966\n",
      "Loss: 2.285475730895996\n",
      "Loss: 1.5800632238388062\n",
      "Loss: 1.7424473762512207\n",
      "Loss: 1.8470573425292969\n",
      "Loss: 1.897160530090332\n",
      "Loss: 2.1805827617645264\n",
      "Loss: 1.6885789632797241\n",
      "Loss: 2.271242380142212\n",
      "Loss: 1.8427882194519043\n",
      "Loss: 1.9387202262878418\n",
      "Loss: 1.5313307046890259\n",
      "Loss: 1.8997386693954468\n",
      "Loss: 2.187620162963867\n",
      "Loss: 1.783787488937378\n",
      "Loss: 1.4956070184707642\n",
      "Loss: 1.3008068799972534\n",
      "Loss: 1.8427925109863281\n",
      "Loss: 1.5291905403137207\n",
      "Loss: 2.2498626708984375\n",
      "Loss: 1.9374223947525024\n",
      "Loss: 1.7712047100067139\n",
      "Loss: 2.273674488067627\n",
      "Loss: 2.507751941680908\n",
      "Loss: 1.3492487668991089\n",
      "Loss: 1.3321380615234375\n",
      "Loss: 1.870569109916687\n",
      "Loss: 1.3673806190490723\n",
      "Loss: 1.645963430404663\n",
      "Loss: 1.9692771434783936\n",
      "Loss: 1.763812780380249\n",
      "Loss: 1.3810335397720337\n",
      "Loss: 2.039816379547119\n",
      "Loss: 1.8644094467163086\n",
      "Loss: 1.3495692014694214\n",
      "Loss: 1.7462226152420044\n",
      "Loss: 1.5179191827774048\n",
      "Loss: 2.4805798530578613\n",
      "Loss: 1.7695811986923218\n",
      "Loss: 1.279304027557373\n",
      "Loss: 1.6276967525482178\n",
      "Loss: 1.78652024269104\n",
      "Loss: 1.390986680984497\n",
      "Loss: 1.7292816638946533\n",
      "Loss: 1.6692888736724854\n",
      "Loss: 2.1228127479553223\n",
      "Loss: 1.4360218048095703\n",
      "Loss: 1.5638786554336548\n",
      "Loss: 1.3575164079666138\n",
      "Loss: 2.544404983520508\n",
      "Loss: 2.0459084510803223\n",
      "Loss: 2.723928928375244\n",
      "Loss: 1.6067873239517212\n",
      "Loss: 1.642978549003601\n",
      "Loss: 1.3293405771255493\n",
      "Loss: 1.8635749816894531\n",
      "Loss: 1.3978643417358398\n",
      "Loss: 2.251991033554077\n",
      "Loss: 1.7634034156799316\n",
      "Loss: 1.7638213634490967\n",
      "Loss: 1.3028558492660522\n",
      "Loss: 2.2618801593780518\n",
      "Loss: 2.3747756481170654\n",
      "Loss: 2.089095115661621\n",
      "Loss: 1.3975056409835815\n",
      "Loss: 1.6466295719146729\n",
      "Loss: 1.417913556098938\n",
      "Loss: 1.1482138633728027\n",
      "Loss: 1.6859914064407349\n",
      "Loss: 2.1363229751586914\n",
      "Loss: 2.2324063777923584\n",
      "Loss: 1.5882868766784668\n",
      "Loss: 1.4340137243270874\n",
      "Loss: 1.682766318321228\n",
      "Loss: 1.3721308708190918\n",
      "Loss: 1.8067365884780884\n",
      "Loss: 2.17592191696167\n",
      "Loss: 1.594512701034546\n",
      "Loss: 2.2540059089660645\n",
      "Loss: 2.03218412399292\n",
      "Loss: 2.2748758792877197\n",
      "Loss: 1.666769027709961\n",
      "Loss: 1.7923026084899902\n",
      "Loss: 2.084712266921997\n",
      "Loss: 1.8924756050109863\n",
      "Loss: 1.8539385795593262\n",
      "Loss: 1.873958706855774\n",
      "Loss: 1.7932584285736084\n",
      "Loss: 1.8188815116882324\n",
      "Loss: 2.471679925918579\n",
      "Loss: 2.0590858459472656\n",
      "Loss: 1.661311149597168\n",
      "Loss: 2.08551025390625\n",
      "Loss: 2.0062804222106934\n",
      "Loss: 1.623197078704834\n",
      "Loss: 1.4465523958206177\n",
      "Loss: 1.4598819017410278\n",
      "Loss: 0.9501714706420898\n",
      "Loss: 1.3378268480300903\n",
      "Loss: 2.132810115814209\n",
      "Loss: 1.959795594215393\n",
      "Loss: 1.5386916399002075\n",
      "Loss: 1.606247067451477\n",
      "Loss: 1.699446678161621\n",
      "Loss: 1.4182378053665161\n",
      "Loss: 1.891423225402832\n",
      "Loss: 1.8066914081573486\n",
      "Loss: 1.6004042625427246\n",
      "Loss: 1.8833917379379272\n",
      "Loss: 1.5378496646881104\n",
      "Loss: 1.5352379083633423\n",
      "Loss: 1.7023706436157227\n",
      "Loss: 2.2471609115600586\n",
      "Loss: 1.7738441228866577\n",
      "Loss: 1.7803311347961426\n",
      "Loss: 1.4523236751556396\n",
      "Loss: 1.8985919952392578\n",
      "Loss: 1.9394896030426025\n",
      "Loss: 1.8808292150497437\n",
      "Loss: 1.4760645627975464\n",
      "Loss: 1.1698344945907593\n",
      "Loss: 2.3197124004364014\n",
      "Loss: 2.0300817489624023\n",
      "Loss: 1.8074322938919067\n",
      "Loss: 1.1307896375656128\n",
      "Loss: 1.6201742887496948\n",
      "Loss: 1.4057055711746216\n",
      "Loss: 2.0686776638031006\n",
      "Loss: 1.911476492881775\n",
      "Loss: 1.893108606338501\n",
      "Loss: 1.1630547046661377\n",
      "Epoch 0 Loss: 1.1630547046661377\n",
      "Epoch: 1\n",
      "Loss: 1.5901051759719849\n",
      "Loss: 2.223945379257202\n",
      "Loss: 1.7060270309448242\n",
      "Loss: 2.254965305328369\n",
      "Loss: 1.7026565074920654\n",
      "Loss: 1.526323914527893\n",
      "Loss: 1.9490673542022705\n",
      "Loss: 1.4264912605285645\n",
      "Loss: 2.024507522583008\n",
      "Loss: 1.7682294845581055\n",
      "Loss: 1.4289337396621704\n",
      "Loss: 2.1071321964263916\n",
      "Loss: 1.36348557472229\n",
      "Loss: 1.3198038339614868\n",
      "Loss: 1.5856692790985107\n",
      "Loss: 1.606893539428711\n",
      "Loss: 1.8270792961120605\n",
      "Loss: 2.1957342624664307\n",
      "Loss: 1.7412105798721313\n",
      "Loss: 1.8537338972091675\n",
      "Loss: 1.2880009412765503\n",
      "Loss: 1.2690294981002808\n",
      "Loss: 1.5810680389404297\n",
      "Loss: 1.69762122631073\n",
      "Loss: 1.4592187404632568\n",
      "Loss: 1.432668924331665\n",
      "Loss: 1.7398743629455566\n",
      "Loss: 1.1951240301132202\n",
      "Loss: 1.8626060485839844\n",
      "Loss: 1.8246592283248901\n",
      "Loss: 1.6601437330245972\n",
      "Loss: 2.2193286418914795\n",
      "Loss: 1.3757851123809814\n",
      "Loss: 1.1536765098571777\n",
      "Loss: 1.596200704574585\n",
      "Loss: 1.5310101509094238\n",
      "Loss: 1.5121880769729614\n",
      "Loss: 1.580843448638916\n",
      "Loss: 1.9452333450317383\n",
      "Loss: 1.8183729648590088\n",
      "Loss: 1.6836133003234863\n",
      "Loss: 1.9069873094558716\n",
      "Loss: 1.4668534994125366\n",
      "Loss: 1.2152646780014038\n",
      "Loss: 2.273794174194336\n",
      "Loss: 1.4945303201675415\n",
      "Loss: 1.5613890886306763\n",
      "Loss: 1.8390083312988281\n",
      "Loss: 1.2835427522659302\n",
      "Loss: 1.3409887552261353\n",
      "Loss: 2.413135051727295\n",
      "Loss: 1.857788324356079\n",
      "Loss: 1.3944536447525024\n",
      "Loss: 1.6091241836547852\n",
      "Loss: 1.5534168481826782\n",
      "Loss: 1.8564561605453491\n",
      "Loss: 1.7719932794570923\n",
      "Loss: 1.386282205581665\n",
      "Loss: 1.4042088985443115\n",
      "Loss: 1.791013240814209\n",
      "Loss: 1.8156650066375732\n",
      "Loss: 1.5581283569335938\n",
      "Loss: 1.243586540222168\n",
      "Loss: 1.5283278226852417\n",
      "Loss: 2.3346805572509766\n",
      "Loss: 1.3041112422943115\n",
      "Loss: 2.1636390686035156\n",
      "Loss: 2.1884877681732178\n",
      "Loss: 1.5697839260101318\n",
      "Loss: 1.8441872596740723\n",
      "Loss: 1.4443215131759644\n",
      "Loss: 0.9839751720428467\n",
      "Loss: 1.6705666780471802\n",
      "Loss: 1.9132241010665894\n",
      "Loss: 0.9492692351341248\n",
      "Loss: 1.180627465248108\n",
      "Loss: 1.049328088760376\n",
      "Loss: 1.1988201141357422\n",
      "Loss: 1.2592031955718994\n",
      "Loss: 1.2445183992385864\n",
      "Loss: 2.125704050064087\n",
      "Loss: 1.690126895904541\n",
      "Loss: 1.1868609189987183\n",
      "Loss: 1.7778760194778442\n",
      "Loss: 1.3894892930984497\n",
      "Loss: 1.9395843744277954\n",
      "Loss: 1.3005181550979614\n",
      "Loss: 1.4734631776809692\n",
      "Loss: 1.3567982912063599\n",
      "Loss: 1.6700100898742676\n",
      "Loss: 1.7694097757339478\n",
      "Loss: 1.217040777206421\n",
      "Loss: 1.8144865036010742\n",
      "Loss: 2.084672212600708\n",
      "Loss: 1.6232900619506836\n",
      "Loss: 1.352640151977539\n",
      "Loss: 2.1394495964050293\n",
      "Loss: 1.3082133531570435\n",
      "Loss: 1.6788984537124634\n",
      "Loss: 1.3764879703521729\n",
      "Loss: 1.2646747827529907\n",
      "Loss: 1.925647497177124\n",
      "Loss: 1.3153127431869507\n",
      "Loss: 1.6109447479248047\n",
      "Loss: 2.0463404655456543\n",
      "Loss: 1.4586379528045654\n",
      "Loss: 1.33310067653656\n",
      "Loss: 1.3165708780288696\n",
      "Loss: 1.1613401174545288\n",
      "Loss: 1.4547982215881348\n",
      "Loss: 1.8699495792388916\n",
      "Loss: 2.1320152282714844\n",
      "Loss: 1.4574432373046875\n",
      "Loss: 1.5413479804992676\n",
      "Loss: 1.6487526893615723\n",
      "Loss: 1.608718991279602\n",
      "Loss: 1.7949528694152832\n",
      "Loss: 1.7360726594924927\n",
      "Loss: 1.6951632499694824\n",
      "Loss: 1.3749667406082153\n",
      "Loss: 1.1379865407943726\n",
      "Loss: 1.3257272243499756\n",
      "Loss: 1.540232539176941\n",
      "Loss: 1.5678901672363281\n",
      "Loss: 1.6175118684768677\n",
      "Loss: 1.4249727725982666\n",
      "Loss: 1.5606648921966553\n",
      "Loss: 1.5161958932876587\n",
      "Loss: 1.822297215461731\n",
      "Loss: 2.1714565753936768\n",
      "Loss: 1.4300377368927002\n",
      "Loss: 1.380831241607666\n",
      "Loss: 1.4928280115127563\n",
      "Loss: 1.8837180137634277\n",
      "Loss: 1.67900550365448\n",
      "Loss: 2.209552764892578\n",
      "Loss: 1.9620027542114258\n",
      "Loss: 1.5987707376480103\n",
      "Loss: 2.067845344543457\n",
      "Loss: 2.286746025085449\n",
      "Loss: 1.265122890472412\n",
      "Loss: 1.4067193269729614\n",
      "Loss: 1.8990267515182495\n",
      "Loss: 1.8490501642227173\n",
      "Loss: 1.48232102394104\n",
      "Loss: 1.9565237760543823\n",
      "Loss: 1.7757540941238403\n",
      "Loss: 1.6372724771499634\n",
      "Loss: 1.4153146743774414\n",
      "Loss: 1.5830919742584229\n",
      "Loss: 1.8802063465118408\n",
      "Loss: 1.563376545906067\n",
      "Loss: 2.3178393840789795\n",
      "Loss: 1.4378224611282349\n",
      "Loss: 2.0707995891571045\n",
      "Loss: 1.7615488767623901\n",
      "Loss: 2.0676991939544678\n",
      "Loss: 1.40752375125885\n",
      "Loss: 1.5475645065307617\n",
      "Loss: 1.368936538696289\n",
      "Loss: 1.6760058403015137\n",
      "Loss: 1.5608705282211304\n",
      "Loss: 1.449884057044983\n",
      "Loss: 1.6723185777664185\n",
      "Loss: 2.0894174575805664\n",
      "Loss: 1.5160671472549438\n",
      "Loss: 1.7730724811553955\n",
      "Loss: 1.8572463989257812\n",
      "Loss: 1.4248855113983154\n",
      "Loss: 1.9110454320907593\n",
      "Loss: 1.5588572025299072\n",
      "Loss: 2.170588493347168\n",
      "Loss: 1.584302306175232\n",
      "Loss: 1.5821595191955566\n",
      "Loss: 1.5653573274612427\n",
      "Loss: 1.5375953912734985\n",
      "Loss: 1.5398749113082886\n",
      "Loss: 1.7152256965637207\n",
      "Loss: 1.4912482500076294\n",
      "Loss: 1.9560047388076782\n",
      "Loss: 2.0610337257385254\n",
      "Loss: 1.2845823764801025\n",
      "Loss: 1.8912526369094849\n",
      "Loss: 1.7620033025741577\n",
      "Loss: 1.4933959245681763\n",
      "Loss: 2.392153263092041\n",
      "Loss: 1.0386873483657837\n",
      "Loss: 1.5859004259109497\n",
      "Loss: 1.9974960088729858\n",
      "Loss: 1.202125072479248\n",
      "Loss: 1.0307143926620483\n",
      "Loss: 1.8487634658813477\n",
      "Loss: 1.401319146156311\n",
      "Loss: 0.8861376047134399\n",
      "Loss: 1.6054555177688599\n",
      "Loss: 1.7757402658462524\n",
      "Loss: 1.6924703121185303\n",
      "Loss: 1.4153850078582764\n",
      "Loss: 1.7574760913848877\n",
      "Loss: 1.6462503671646118\n",
      "Loss: 1.4041386842727661\n",
      "Loss: 1.5043131113052368\n",
      "Loss: 1.755867600440979\n",
      "Loss: 1.379763126373291\n",
      "Loss: 1.5979294776916504\n",
      "Loss: 2.082092046737671\n",
      "Loss: 2.078089952468872\n",
      "Loss: 1.5020049810409546\n",
      "Loss: 1.7281783819198608\n",
      "Loss: 1.314706802368164\n",
      "Loss: 1.263776183128357\n",
      "Loss: 2.136321544647217\n",
      "Loss: 2.1598548889160156\n",
      "Loss: 1.7529569864273071\n",
      "Loss: 1.4253506660461426\n",
      "Loss: 1.9650061130523682\n",
      "Loss: 1.5649577379226685\n",
      "Loss: 1.4439806938171387\n",
      "Loss: 2.069485902786255\n",
      "Loss: 1.874800682067871\n",
      "Loss: 1.245505928993225\n",
      "Loss: 1.9985904693603516\n",
      "Loss: 2.1549510955810547\n",
      "Loss: 1.9168884754180908\n",
      "Loss: 1.6713827848434448\n",
      "Loss: 1.7670871019363403\n",
      "Loss: 1.6116809844970703\n",
      "Loss: 2.1147336959838867\n",
      "Loss: 1.1049288511276245\n",
      "Loss: 2.107893466949463\n",
      "Loss: 1.096000075340271\n",
      "Loss: 2.1229798793792725\n",
      "Loss: 1.6337387561798096\n",
      "Loss: 1.4480068683624268\n",
      "Loss: 1.6532397270202637\n",
      "Loss: 1.633681058883667\n",
      "Loss: 1.492808222770691\n",
      "Loss: 1.6613264083862305\n",
      "Loss: 1.2709496021270752\n",
      "Loss: 1.6049572229385376\n",
      "Loss: 1.7833349704742432\n",
      "Loss: 1.4662479162216187\n",
      "Loss: 1.8914958238601685\n",
      "Loss: 1.8461432456970215\n",
      "Loss: 1.564813256263733\n",
      "Loss: 1.715823769569397\n",
      "Loss: 1.9769054651260376\n",
      "Loss: 1.61935293674469\n",
      "Loss: 2.048360586166382\n",
      "Loss: 1.752602219581604\n",
      "Loss: 1.4792946577072144\n",
      "Loss: 1.1829066276550293\n",
      "Loss: 2.089938163757324\n",
      "Loss: 2.070890188217163\n",
      "Loss: 1.589545488357544\n",
      "Loss: 1.2084397077560425\n",
      "Loss: 1.272256851196289\n",
      "Loss: 1.0229191780090332\n",
      "Loss: 1.556383490562439\n",
      "Loss: 1.5334293842315674\n",
      "Loss: 1.415663242340088\n",
      "Loss: 1.1760190725326538\n",
      "Loss: 1.533576488494873\n",
      "Loss: 1.6876710653305054\n",
      "Loss: 1.2368921041488647\n",
      "Loss: 1.202580451965332\n",
      "Loss: 1.6175389289855957\n",
      "Loss: 1.5696943998336792\n",
      "Loss: 1.2024449110031128\n",
      "Loss: 1.9054186344146729\n",
      "Loss: 1.9942293167114258\n",
      "Loss: 1.9109539985656738\n",
      "Loss: 1.95592200756073\n",
      "Loss: 1.8003718852996826\n",
      "Loss: 1.5815361738204956\n",
      "Loss: 1.8291209936141968\n",
      "Loss: 1.8738572597503662\n",
      "Loss: 2.1027991771698\n",
      "Loss: 1.4607985019683838\n",
      "Loss: 1.1816349029541016\n",
      "Loss: 1.9824943542480469\n",
      "Loss: 1.5615475177764893\n",
      "Loss: 1.3682409524917603\n",
      "Loss: 1.3527103662490845\n",
      "Loss: 1.5773507356643677\n",
      "Loss: 1.705925464630127\n",
      "Loss: 1.4046622514724731\n",
      "Loss: 1.1117725372314453\n",
      "Loss: 1.6176420450210571\n",
      "Loss: 1.5292788743972778\n",
      "Loss: 1.6627702713012695\n",
      "Loss: 1.6621280908584595\n",
      "Loss: 1.19907546043396\n",
      "Loss: 1.3493123054504395\n",
      "Loss: 2.1056106090545654\n",
      "Loss: 1.326024055480957\n",
      "Loss: 1.7930352687835693\n",
      "Loss: 1.4256248474121094\n",
      "Loss: 1.4835935831069946\n",
      "Loss: 1.7287633419036865\n",
      "Loss: 1.4401423931121826\n",
      "Loss: 1.6772269010543823\n",
      "Loss: 1.5616967678070068\n",
      "Loss: 2.053690195083618\n",
      "Loss: 1.7302006483078003\n",
      "Loss: 1.9356218576431274\n",
      "Loss: 1.1222119331359863\n",
      "Loss: 1.68388831615448\n",
      "Loss: 1.9119495153427124\n",
      "Loss: 1.8122202157974243\n",
      "Loss: 1.8855974674224854\n",
      "Loss: 1.8493973016738892\n",
      "Loss: 1.4989498853683472\n",
      "Loss: 1.9432116746902466\n",
      "Loss: 1.7278540134429932\n",
      "Loss: 1.6899257898330688\n",
      "Loss: 1.3411375284194946\n",
      "Loss: 1.6514562368392944\n",
      "Loss: 1.5675647258758545\n",
      "Loss: 1.300546646118164\n",
      "Loss: 1.634263038635254\n",
      "Loss: 1.9297200441360474\n",
      "Loss: 1.387937307357788\n",
      "Loss: 1.9460891485214233\n",
      "Loss: 1.3762024641036987\n",
      "Loss: 1.4854708909988403\n",
      "Loss: 1.564035415649414\n",
      "Loss: 1.1240901947021484\n",
      "Loss: 1.2019927501678467\n",
      "Loss: 1.4703428745269775\n",
      "Loss: 1.9820284843444824\n",
      "Loss: 1.4701827764511108\n",
      "Loss: 1.332376480102539\n",
      "Loss: 1.7792216539382935\n",
      "Loss: 1.5729533433914185\n",
      "Loss: 1.4651966094970703\n",
      "Loss: 1.2247587442398071\n",
      "Loss: 1.4278991222381592\n",
      "Loss: 1.254089593887329\n",
      "Loss: 1.4958970546722412\n",
      "Loss: 1.4475960731506348\n",
      "Loss: 1.8470791578292847\n",
      "Loss: 1.4957880973815918\n",
      "Loss: 0.8219965100288391\n",
      "Loss: 2.1422054767608643\n",
      "Loss: 1.2662996053695679\n",
      "Loss: 1.5456886291503906\n",
      "Loss: 2.026998519897461\n",
      "Loss: 2.153629779815674\n",
      "Loss: 1.7555360794067383\n",
      "Loss: 1.439420223236084\n",
      "Loss: 1.3175147771835327\n",
      "Loss: 1.752236008644104\n",
      "Loss: 1.1171749830245972\n",
      "Loss: 1.9186973571777344\n",
      "Loss: 1.3003631830215454\n",
      "Loss: 1.3205897808074951\n",
      "Loss: 1.2816646099090576\n",
      "Loss: 1.1670112609863281\n",
      "Loss: 1.1322791576385498\n",
      "Loss: 1.4043769836425781\n",
      "Loss: 1.3701890707015991\n",
      "Loss: 1.8523999452590942\n",
      "Loss: 1.6855756044387817\n",
      "Loss: 1.3625438213348389\n",
      "Loss: 1.9649156332015991\n",
      "Loss: 2.1194331645965576\n",
      "Loss: 1.4274425506591797\n",
      "Loss: 2.0735435485839844\n",
      "Loss: 1.600381851196289\n",
      "Loss: 1.531758427619934\n",
      "Loss: 1.7386529445648193\n",
      "Loss: 1.9831266403198242\n",
      "Loss: 1.6837458610534668\n",
      "Loss: 1.675902009010315\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(training_model.parameters(), lr=5e-4)\n",
    "# loss = torch.nn.CosineEmbeddingLoss()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "training_model.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "  print(\"Epoch:\", epoch)\n",
    "  for idx, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch.pop(\"input_ids\").to(device)\n",
    "    pixel_values = batch.pop(\"pixel_values\").to(device, torch.float32)\n",
    "    outputs = training_model(input_ids=input_ids,\n",
    "                    pixel_values=pixel_values,\n",
    "                    labels=input_ids)\n",
    "    \n",
    "    loss = outputs.loss\n",
    "\n",
    "    print(\"Loss:\", loss.item())\n",
    "      \n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "  print(f\"Epoch {epoch} Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"images/1.png\")\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float32)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "generated_ids = training_model.generate(pixel_values=pixel_values, max_length=30)\n",
    "generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(training_model.state_dict(), \"model_v3.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
